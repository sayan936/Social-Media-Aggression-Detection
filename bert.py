# -*- coding: utf-8 -*-
"""BERT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1e2pWRD19CMUnTWYNviIS2tt6RV0tsgw0
"""

gpu_info = !nvidia-smi
gpu_info = '\n'.join(gpu_info)
if gpu_info.find('failed') >= 0:
  print('Not connected to a GPU')
else:
  print(gpu_info)

pip install transformers

pip install wordsegment

pip install emoji

pip install contractions

# Commented out IPython magic to ensure Python compatibility.
import re
import emoji
import wordsegment
from wordsegment import load, segment
load()
import numpy as np

import nltk
from nltk.corpus import stopwords
import contractions

import nltk
nltk.download('wordnet')
from nltk.stem import WordNetLemmatizer

import transformers
from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup
import torch

import numpy as np
import pandas as pd
import seaborn as sns
from pylab import rcParams
import matplotlib.pyplot as plt
from matplotlib import rc
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report
from collections import defaultdict
from textwrap import wrap

from torch import nn, optim
from torch.utils.data import Dataset, DataLoader
import torch.nn.functional as F

# %matplotlib inline
# %config InlineBackend.figure_format='retina'

sns.set(style='whitegrid', palette='muted', font_scale=1.2)

HAPPY_COLORS_PALETTE = ["#01BEFE", "#FFDD00", "#FF7D00", "#FF006D", "#ADFF02", "#8F00FF"]

sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))

rcParams['figure.figsize'] = 12, 8

RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)
torch.manual_seed(RANDOM_SEED)

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
device



train = pd.read_csv('/content/drive/MyDrive/agr_en_train.csv', names = ['SOURCE', 'TEXT', 'AGGRESSION_CLASS'])
valid =  pd.read_csv('/content/drive/MyDrive/agr_en_dev.csv', names = ['SOURCE', 'TEXT', 'AGGRESSION_CLASS'])
test = pd.read_csv('/content/drive/MyDrive/agr_en_fb_gold.csv',names =['SOURCE','TEXT','AGGRESSION_CLASS'])



#data = pd.concat([train, valid], ignore_index=True)





ax = sns.countplot(x = 'AGGRESSION_CLASS', data = train)
plt.xlabel('review sentiment')

"""## Data Preprocessing"""

import re
import emoji
import wordsegment
from wordsegment import load, segment
load()

def preprocess_text(text):
  text = text.lower()

  # Replace URL with http
  text = re.sub(r'http\S+', 'http', text)

  # Replace emoticons with corresponding phrases
  text = emoji.emojize(text, language='en')
  text = emoji.demojize(text, language='en', delimiters=(' ', ' '))

  # Segment hashtags
  text = re.sub(r'#(\w+)', lambda m: ' '.join(segment(m.group(1))), text)

  #remove extra white spaces
  text = " ".join(text.split())

  #word_Contractions
  text = contractions.fix(text)

  #word repeat
  text = re.sub(r'(.)\1+', r'\1\1', text)

  # Tokenize text
  words = text.split()

  # Remove stop words
  #stop_words = set(stopwords.words('english'))
  #words = [word for word in words if word.lower() not in stop_words]


  #lemmatize
  lemmatizer = WordNetLemmatizer()
  words = [lemmatizer.lemmatize(word) for word in words]
  words = [lemmatizer.lemmatize(word, "v") for word in words]

  # Remove digits, punctuation, email, and non UTF-8 characters
  words = [re.sub(r'\d+', '', word) for word in words]
  words = [re.sub(r'[^\w\s]+', '', word) for word in words]
  words = [re.sub(r'\S+@\S+', '', word) for word in words]

  # Encode and decode to handle non-UTF-8 characters
  words = [word.encode('ascii', 'ignore').decode('utf-8') for word in words]

  return ' '.join(words)

'''
# Convert 'TEXT' column to strings

data['TEXT'] = data['TEXT'].apply(lambda x: x.lower())
data['TEXT'] = data['TEXT'].astype(str)



# Remove phone numbers
data['TEXT'] = data['TEXT'].apply(lambda x: re.sub(r'\+\d{2}-\d+', '', x))

# Remove stop words
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))
data['TEXT'] = data['TEXT'].apply(lambda x: ' '.join(word for word in x.split() if word.lower() not in stop_words))

# Remove punctuation
data['TEXT'] = data['TEXT'].apply(lambda x: re.sub(r'[^\w\s]', '', x))

# Replace multiple spaces or dots with a single one
data['TEXT'] = data['TEXT'].apply(lambda x: re.sub(r'\s+', ' ', x))
data['TEXT'] = data['TEXT'].apply(lambda x: re.sub(r'\.+', '.', x))

# Remove URLs
data['TEXT'] = data['TEXT'].apply(lambda x: re.sub(r'http\S+|www\S+|https\S+', '', x))

# Emojize and demojize
data['TEXT'] = data['TEXT'].apply(lambda x: emoji.emojize(x, language='en'))
data['TEXT'] = data['TEXT'].apply(lambda x: emoji.demojize(x, language='en', delimiters=(' ', ' ')))
'''

#x = preprocess_comment("This is an example tweet with #hashtags and ðŸ˜ƒ emojis! Check it out http://example.com")
#print(x)

'''
train['TEXT'] = train['TEXT'].apply(preprocess_comment)
valid['TEXT'] = valid['TEXT'].apply(preprocess_comment)
test['TEXT'] = test['TEXT'].apply(preprocess_comment)
'''

train.head()

ax = sns.countplot(x = 'AGGRESSION_CLASS', data = valid)
plt.xlabel('review sentiment')

ax = sns.countplot(x = 'AGGRESSION_CLASS', data =train)
plt.xlabel('review sentiment')

"""## Data Shuffling"""

np.random.seed(42)
train_shuffled = train.reindex(np.random.permutation(train.index))
valid_shuffled = valid.reindex(np.random.permutation(valid.index))

CAG = train_shuffled[train_shuffled['AGGRESSION_CLASS'] == 'CAG']
OAG = train_shuffled[train_shuffled['AGGRESSION_CLASS'] == 'OAG']
NAG = train_shuffled[train_shuffled['AGGRESSION_CLASS'] == 'NAG']

concated_train = pd.concat([CAG, OAG, NAG], ignore_index=True)
concated_train['LABEL'] = 0

np.random.seed(42)
concated_train = concated_train.reindex(np.random.permutation(concated_train.index))


# Class to idx
concated_train.loc[concated_train['AGGRESSION_CLASS'] == 'CAG', 'LABEL'] = 1
concated_train.loc[concated_train['AGGRESSION_CLASS'] == 'OAG', 'LABEL'] = 2
concated_train.loc[concated_train['AGGRESSION_CLASS'] == 'NAG', 'LABEL'] = 0

# Valid
CAG = valid_shuffled[valid_shuffled['AGGRESSION_CLASS'] == 'CAG']
OAG = valid_shuffled[valid_shuffled['AGGRESSION_CLASS'] == 'OAG']
NAG = valid_shuffled[valid_shuffled['AGGRESSION_CLASS'] == 'NAG']

concated_valid = pd.concat([CAG, OAG, NAG], ignore_index=True)
concated_valid['LABEL'] = 0

np.random.seed(42)
concated_valid = concated_valid.reindex(np.random.permutation(concated_valid.index))






# Class to idx
concated_valid.loc[concated_valid['AGGRESSION_CLASS'] == 'CAG', 'LABEL'] = 1
concated_valid.loc[concated_valid['AGGRESSION_CLASS'] == 'OAG', 'LABEL'] = 2
concated_valid.loc[concated_valid['AGGRESSION_CLASS'] == 'NAG', 'LABEL'] = 0


'''
data['LABEL'] = 0
data.loc[data['AGGRESSION_CLASS'] == 'CAG', 'LABEL'] = 1
data.loc[data['AGGRESSION_CLASS'] == 'OAG', 'LABEL'] = 2
data.loc[data['AGGRESSION_CLASS'] == 'NAG', 'LABEL'] = 0
'''

concated_train = concated_train[['TEXT','AGGRESSION_CLASS','LABEL']]
concated_train['TEXT'] = concated_train['TEXT'].apply(preprocess_text)
concated_train

concated_valid = concated_valid[['TEXT','AGGRESSION_CLASS','LABEL']]
concated_valid['TEXT'] = concated_valid['TEXT'].apply(preprocess_text)
concated_valid

class_names = ['NAG', 'CAG', 'OAG']



"""BERT IMPLEMENTATION


"""

from transformers import BertTokenizer, DistilBertModel,AutoTokenizer,BertForSequenceClassification
PRE_TRAINED_MODEL_NAME = 'bert-base-uncased'

#PRE_TRAINED_MODEL_NAME = 'distilbert-base-uncased'

tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)
#tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)



token_lens = []

for txt in concated_train.TEXT :
  tokens = tokenizer.encode(txt, max_length=512)
  token_lens.append(len(tokens))

sns.distplot(token_lens)
plt.xlim([0, 256]);
plt.xlabel('Token count');

MAX_LEN =50

#df_train, df_test = train_test_split(data, test_size=0.1, random_state=RANDOM_SEED)
df_val, df_test = train_test_split(concated_valid, test_size=0.2, random_state=RANDOM_SEED)

class GPReviewDataset(Dataset):

  def __init__(self, reviews, targets, tokenizer, max_len):
    self.reviews = reviews
    self.targets = targets
    self.tokenizer = tokenizer
    self.max_len = max_len

  def __len__(self):
    return len(self.reviews)

  def __getitem__(self, item):
    review = str(self.reviews[item])
    target = self.targets[item]

    encoding = self.tokenizer.encode_plus(
      review,
      add_special_tokens=True,
      max_length=self.max_len,
      return_token_type_ids=False,
       pad_to_max_length=True,
      return_attention_mask=True,
      return_tensors='pt',
    )

    return {
      'review_text': review,
      'input_ids': encoding['input_ids'].flatten(),
      'attention_mask': encoding['attention_mask'].flatten(),
      'targets': torch.tensor(target, dtype=torch.long)
    }



concated_train.shape, df_val.shape, df_test.shape





def create_data_loader(df, tokenizer, max_len, batch_size):
  ds = GPReviewDataset(
    reviews=df.TEXT.to_numpy(),
    targets=df.LABEL.to_numpy(),
    tokenizer=tokenizer,
    max_len=max_len
  )

  return DataLoader(
    ds,
    batch_size=batch_size,
    num_workers=4
  )

BATCH_SIZE = 8

train_data_loader = create_data_loader(concated_train, tokenizer, MAX_LEN, BATCH_SIZE)
val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)
test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)

#data = next(iter(train_data_loader))
#data.keys()

'''
print(data['input_ids'].shape)
print(data['attention_mask'].shape)
print(data['targets'].shape)
'''



"""BERT FOR CLASSIFICATION"""

'''
class SentimentClassifier(nn.Module):

  def __init__(self, n_classes):
    super(SentimentClassifier, self).__init__()
    self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)
    self.dropout = nn.Dropout(p=0.1)
    self.relu = nn.ReLU()
    self.fc1 = nn.Linear(768,512)

    self.out = nn.Linear(512, n_classes)
    self.softmax = nn.LogSoftmax(dim=1)

  def forward(self, input_ids, attention_mask):
    _, pooled_output = self.bert(
      input_ids=input_ids,
      attention_mask=attention_mask,
      return_dict=False

    )
    output = self.fc1(pooled_output)
    output = self.relu(output)

    output = self.dropout(output)

      # output layer
    output = self.out(output)
    output = self.softmax(output)

    return output
'''

#model = SentimentClassifier(len(class_names))


model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(class_names))
#dropout_prob = 0.3  # Set the dropout probability as needed

'''
# Add dropout to the BERT model
#model.distilbert.encoder.layer[-1].output.dropout = torch.nn.Dropout(p=dropout_prob)
#model.classifier.dropout = torch.nn.Dropout(p=dropout_prob)

model = model.to(device)
print(model)
'''

#model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(class_names))
model = model.to(device)

# Define a non-linear activation function (e.g., ReLU)
#activation = nn.ReLU()

# Modify the final classifier layer to include the activation function
#model.classifier = nn.Sequential(
    #model.classifier,
    #activation
#)

print(model)

EPOCHS = 5

optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)
total_steps = len(train_data_loader) * EPOCHS

scheduler = get_linear_schedule_with_warmup(
  optimizer,
  num_warmup_steps= total_steps*0.1,
  num_training_steps=total_steps
)

loss_fn = nn.CrossEntropyLoss().to(device)

concated_train

from sklearn.metrics import accuracy_score

def train_epoch(
  model,
  data_loader,
  loss_fn,
  optimizer,
  device,
  scheduler,
  n_examples
):
  model = model.train()

  losses = []
  correct_predictions = 0

  for d in data_loader:
    input_ids = d["input_ids"].to(device)
    attention_mask = d["attention_mask"].to(device)
    targets = d["targets"].to(device)

    optimizer.zero_grad()
    #print(f"input_ids: {input_ids}")
    #print(f"attention_mask: {attention_mask}")
    #print(f"targets: {targets}")

    outputs = model(
      input_ids=input_ids,
      attention_mask=attention_mask
    ).logits
    #print(outputs)
    #_, preds = torch.max(outputs, dim=1)
    preds = torch.argmax(outputs, dim=1)
    #print(f"preds: {preds}")
    loss = loss_fn(outputs,targets)

    correct_predictions += torch.sum(preds == targets)
    #print(f"correct_pred {correct_predictions}")
    losses.append(loss.item())

    loss.backward()
    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
    optimizer.step()
    scheduler.step()

    #correct_predictions.double() / n_examples

  return correct_predictions.double() / n_examples, np.mean(losses)

def eval_model(model, data_loader, loss_fn, device, n_examples):
  model = model.eval()

  losses = []
  correct_predictions = 0

  with torch.no_grad():
    for d in data_loader:
      input_ids = d["input_ids"].to(device)
      attention_mask = d["attention_mask"].to(device)
      targets = d["targets"].to(device)
      #print(targets)

      outputs = model(
        input_ids=input_ids,
        attention_mask=attention_mask
      ).logits
      #_, preds = torch.max(outputs, dim=1)
      preds = torch.argmax(outputs, dim=1)
      #print(preds)
      #print(preds)
      loss = loss_fn(outputs, targets)

      correct_predictions += torch.sum(preds == targets)
      losses.append(loss.item())

      #correct_predictions.double() / n_examples

  return correct_predictions.double() / n_examples, np.mean(losses)



history = defaultdict(list)
best_accuracy = 0

for epoch in range(EPOCHS):

  print(f'Epoch {epoch + 1}/{EPOCHS}')
  print('-' * 10)

  train_acc, train_loss = train_epoch(
    model,
    train_data_loader,
    loss_fn,
    optimizer,
    device,
    scheduler,
    len(concated_train)
  )

  print(f'Train loss {train_loss} accuracy {train_acc}')

  val_acc, val_loss = eval_model(
    model,
    val_data_loader,
    loss_fn,
    device,
    len(df_val)
  )

  print(f'Val   loss {val_loss} accuracy {val_acc}')
  print()

  history['train_acc'].append(train_acc)
  history['train_loss'].append(train_loss)
  history['val_acc'].append(val_acc)
  history['val_loss'].append(val_loss)

  if val_acc > best_accuracy:
    torch.save(model.state_dict(), 'best_model_state.bin')
    best_accuracy = val_acc

train_acc_cpu = [acc.cpu() for acc in history['train_acc']]
val_acc_cpu = [acc.cpu() for acc in history['val_acc']]
plt.plot(train_acc_cpu, label='train accuracy')
plt.plot(val_acc_cpu, label='validation accuracy')

plt.title('Training history')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend()
plt.ylim([0, 1])

plt.show()

test_acc, _ = eval_model(
  model,
  val_data_loader,
  loss_fn,
  device,
  len(df_test)
)

test_acc.item()

def get_predictions(model, data_loader):
  model = model.eval()

  review_texts = []
  predictions = []
  prediction_probs = []
  real_values = []

  with torch.no_grad():
    for d in data_loader:

      texts = d["review_text"]
      input_ids = d["input_ids"].to(device)
      attention_mask = d["attention_mask"].to(device)
      targets = d["targets"].to(device)

      outputs = model(
        input_ids=input_ids,
        attention_mask=attention_mask
      ).logits
      preds = torch.argmax(outputs, dim=1)
      #print(preds)
      #_, preds = torch.max(outputs, dim=1)

      probs = F.softmax(outputs, dim=1)

      review_texts.extend(texts)
      predictions.extend(preds)
      prediction_probs.extend(probs)
      real_values.extend(targets)

  predictions = torch.stack(predictions).cpu()
  prediction_probs = torch.stack(prediction_probs).cpu()
  real_values = torch.stack(real_values).cpu()
  return review_texts, predictions, prediction_probs, real_values

y_review_texts, y_pred, y_pred_probs, y_test = get_predictions(
  model,
  test_data_loader
)

print(classification_report(y_test, y_pred, target_names=class_names))

def show_confusion_matrix(confusion_matrix):
  hmap = sns.heatmap(confusion_matrix, annot=True, fmt="d", cmap="Blues")
  hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')
  hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')
  plt.ylabel('True sentiment')
  plt.xlabel('Predicted sentiment');

cm = confusion_matrix(y_test, y_pred)
df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)
show_confusion_matrix(df_cm)

concated_valid.AGGRESSION_CLASS.value_counts()

review_text ="Well said sonu..you have courage to stand against dadagiri of Muslims"

encoded_review = tokenizer.encode_plus(
  review_text,
  max_length=MAX_LEN,
  add_special_tokens=True,
  return_token_type_ids=False,
  pad_to_max_length=True,
  return_attention_mask=True,
  return_tensors='pt',
)

input_ids = encoded_review['input_ids'].to(device)
attention_mask = encoded_review['attention_mask'].to(device)

output = model(input_ids, attention_mask).logits
prediction = torch.argmax(output, dim=1)

print(f'Review text: {review_text}')
print(f'Sentiment  : {class_names[prediction]}')

|