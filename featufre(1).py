# -*- coding: utf-8 -*-
"""FEATUFRE.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EOqYCCsQUTS0n4ht7UXjEuuSvoIZQi_H
"""

pip install wordsegment

pip install emoji

pip install contractions;

pip install indic-transliteration



import re
import emoji
import wordsegment
from wordsegment import load, segment
load()
import numpy as np
import string

import nltk
from nltk.corpus import stopwords
import contractions

import nltk
nltk.download('wordnet')
from nltk.stem import WordNetLemmatizer

nltk.download('stopwords')

from indic_transliteration import sanscript
from indic_transliteration.sanscript import transliterate

import nltk
nltk.download('averaged_perceptron_tagger')


import pandas as pd
from sklearn.model_selection import train_test_split


from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, SpatialDropout1D
from tensorflow.keras.optimizers import Adam
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from keras.utils.np_utils import to_categorical
import matplotlib.pyplot as plt

train = pd.read_csv('/content/drive/MyDrive/agr_en_train.csv', names = ['SOURCE', 'TEXT', 'AGGRESSION_CLASS'])
valid =  pd.read_csv('/content/drive/MyDrive/agr_en_dev.csv', names = ['SOURCE', 'TEXT', 'AGGRESSION_CLASS'])

train = train[['TEXT','AGGRESSION_CLASS']]
valid = valid[['TEXT','AGGRESSION_CLASS']]

np.random.seed(42)
train_shuffled = train.reindex(np.random.permutation(train.index))
valid_shuffled = valid.reindex(np.random.permutation(valid.index))

CAG = train_shuffled[train_shuffled['AGGRESSION_CLASS'] == 'CAG']
OAG = train_shuffled[train_shuffled['AGGRESSION_CLASS'] == 'OAG']
NAG = train_shuffled[train_shuffled['AGGRESSION_CLASS'] == 'NAG']

concated_train = pd.concat([CAG, OAG, NAG], ignore_index=True)
concated_train['LABEL'] = 0

np.random.seed(42)
concated_train = concated_train.reindex(np.random.permutation(concated_train.index))


# Class to idx
concated_train.loc[concated_train['AGGRESSION_CLASS'] == 'CAG', 'LABEL'] = 1
concated_train.loc[concated_train['AGGRESSION_CLASS'] == 'OAG', 'LABEL'] = 2
concated_train.loc[concated_train['AGGRESSION_CLASS'] == 'NAG', 'LABEL'] = 0

# Valid
CAG = valid_shuffled[valid_shuffled['AGGRESSION_CLASS'] == 'CAG']
OAG = valid_shuffled[valid_shuffled['AGGRESSION_CLASS'] == 'OAG']
NAG = valid_shuffled[valid_shuffled['AGGRESSION_CLASS'] == 'NAG']

concated_valid = pd.concat([CAG, OAG, NAG], ignore_index=True)
concated_valid['LABEL'] = 0

np.random.seed(42)
concated_valid = concated_valid.reindex(np.random.permutation(concated_valid.index))






# Class to idx
concated_valid.loc[concated_valid['AGGRESSION_CLASS'] == 'CAG', 'LABEL'] = 1
concated_valid.loc[concated_valid['AGGRESSION_CLASS'] == 'OAG', 'LABEL'] = 2
concated_valid.loc[concated_valid['AGGRESSION_CLASS'] == 'NAG', 'LABEL'] = 0

concated_train

"""## **TEXT PREPROCESSING AND FEATURE EXTRACTION **"""



'''
from indic_transliteration import sanscript
from indic_transliteration.sanscript import transliterate

def preprocess_text(text):
  text = text.lower()
  #transliterator = Transliterator(source='hi', target='en', build_lookup=True)
    # Replace URL with http
  text = re.sub(r'http\S+', 'http', text)

  # Replace emoticons with corresponding phrases
  text = emoji.emojize(text, language='en')
  text = emoji.demojize(text, language='en', delimiters=(' ', ' '))

  # Segment hashtags
  text = re.sub(r'#(\w+)', lambda m: ' '.join(segment(m.group(1))), text)

  #remove extra white spaces
  text = " ".join(text.split())

  #word_Contractions
  text = contractions.fix(text)

  # Tokenize text
  words = text.split()

  # Remove stop words
  stop_words = set(stopwords.words('english'))
  words = [word for word in words if word.lower() not in stop_words]


  #lemmatize
  lemmatizer = WordNetLemmatizer()
  words = [lemmatizer.lemmatize(word) for word in words]
  words = [lemmatizer.lemmatize(word, "v") for word in words]

  # Remove digits, punctuation, email, and non UTF-8 characters
  words = [re.sub(r'\d+', '', word) for word in words]
  words = [re.sub(r'[^\w\s]+', '', word) for word in words]
  words = [re.sub(r'\S+@\S+', '', word) for word in words]

  # Transliterate Hindi text to Roman
  for i, word in enumerate(words):
    if any(c in sanscript.DEVANAGARI for c in word):
      roman_word = transliterate(word, sanscript.DEVANAGARI, sanscript.ITRANS)
      words[i] = roman_word

  sentence = ' '.join(words)

  # Transliterate Hindi text to Roman
  roman_text = transliterate(sentence, sanscript.DEVANAGARI, sanscript.ITRANS)

  # Encode and decode to handle non-UTF-8 characters
  roman_text = roman_text.encode('ascii', 'ignore').decode('utf-8')

  return roman_text

  # Encode and decode to handle non-UTF-8 characters
  #words = [word.encode('ascii', 'ignore').decode('utf-8') for word in words]

  #return ' '.join(words)
  '''

from indic_transliteration import sanscript
from indic_transliteration.sanscript import transliterate

def preprocess_text(text):
    text = text.lower()
      #transliterator = Transliterator(source='hi', target='en', build_lookup=True)
        # Replace URL with http
    text = re.sub(r'http\S+', 'http', text)

      # Replace emoticons with corresponding phrases
    text = emoji.emojize(text, language='en')
    text = emoji.demojize(text, language='en', delimiters=(' ', ' '))

      # Segment hashtags
    text = re.sub(r'#(\w+)', lambda m: ' '.join(segment(m.group(1))), text)

      #remove extra white spaces
    text = " ".join(text.split())

      #word_Contractions
    text = contractions.fix(text)

      # Tokenize text
    words = text.split()

      # Remove stop words
    stop_words = set(stopwords.words('english'))
    words = [word for word in words if word.lower() not in stop_words]


      #lemmatize
    lemmatizer = WordNetLemmatizer()
    words = [lemmatizer.lemmatize(word) for word in words]
    words = [lemmatizer.lemmatize(word, "v") for word in words]

    pos_tags = nltk.pos_tag(words)

    # Extract the POS tags from the tagged words
    pos_tags = [tag for _, tag in pos_tags]

    # Join the POS tags with the words
    words= [word + '_' + tag for word, tag in zip(words, pos_tags)]


      # Remove digits, punctuation, email, and non UTF-8 characters
    words = [re.sub(r'\d+', '', word) for word in words]
    words = [re.sub(r'[^\w\s]+', '', word) for word in words]
    words = [re.sub(r'\S+@\S+', '', word) for word in words]

      # Transliterate Hindi text to Roman
    for i, word in enumerate(words):
        if any(c in sanscript.DEVANAGARI for c in word):
            roman_word = transliterate(word, sanscript.DEVANAGARI, sanscript.ITRANS)
            words[i] = roman_word

    sentence = ' '.join(words)

      # Transliterate Hindi text to Roman
    roman_text = transliterate(sentence, sanscript.DEVANAGARI, sanscript.ITRANS)

      # Encode and decode to handle non-UTF-8 characters
    roman_text = roman_text.encode('ascii', 'ignore').decode('utf-8')

    return roman_text

abuses = []

file_path = '/content/badWords.txt'
with open(file_path) as f:
    for word in f:
        word = word.strip('\n')
        abuses.append(word.lower())



pwd

def extract_features(preprocessed_text):
    # Preprocess the input text
    #preprocessed_text = preprocess_text(input_text)

    # Count of abusive/aggressive/offensive words
    #abusive_words = ["abusive_word1", "abusive_word2", "abusive_word3"]  # Replace with your list of abusive words
    abusive_word_count = sum(1 for word in preprocessed_text.split() if word in abuses)


    # Number of tokens
    num_tokens = len(preprocessed_text.split())

    # Size of post/comment (number of characters)
    post_size = len(preprocessed_text)

    # Presence of URLs
    has_urls = 1 if re.search(r'http\S+', preprocessed_text) else 0

    # Presence of phone numbers (a simple example)
    has_phone_numbers = 1 if re.search(r'\d{10}', preprocessed_text) else 0

    # Presence of hash-tags
    has_hashtags = 1 if re.search(r'#\w+', preprocessed_text) else 0

    # Number of single letters
    single_letter_count = sum(1 for word in preprocessed_text.split() if len(word) == 1)

    # Average length of words
    words = preprocessed_text.split()
    avg_word_length = sum(len(word) for word in words) / len(words)

    # Number of words with uppercase characters
    uppercase_word_count = sum(1 for word in words if any(c.isupper() for c in word))

    # Number of Punctuation
    punctuation_count = sum(1 for char in preprocessed_text if char in string.punctuation)

    # ... add more features ...

    # Return the extracted features as a list
    return {
        "abusive_word_count": abusive_word_count,
        "num_tokens": num_tokens,
        "post_size": post_size,
        "has_urls": has_urls,
        "has_phone_numbers": has_phone_numbers,
        "has_hashtags": has_hashtags,
        "single_letter_count": single_letter_count,
        "avg_word_length": avg_word_length,
        "uppercase_word_count": uppercase_word_count,
        "punctuation_count": punctuation_count,
        # ... add more features ...
    }

# Create a new DataFrame to store the extracted features
extracted_features_train = pd.DataFrame()

for index, row in concated_train.iterrows():
    preprocessed_text = row['TEXT']
    features = extract_features(preprocessed_text)

    # Append the features dictionary to the DataFrame
    extracted_features_train = pd.concat([extracted_features_train, pd.DataFrame([features])], ignore_index=True)

# Concatenate the extracted features DataFrame with the original DataFrame
concated_train_with_features = pd.concat([concated_train, extracted_features_train], axis=1)

# Preprocess the text after adding feature columns
for index, row in concated_train_with_features.iterrows():
    preprocessed_text_after_features = preprocess_text(row['TEXT'])
    concated_train_with_features.at[index, 'preprocessed_text_after_features'] = preprocessed_text_after_features

# Drop the original 'TEXT' column
concated_train_with_features.drop(columns=['TEXT'], inplace=True)

# Rename the 'preprocessed_text_after_features' column to 'TEXT'
concated_train_with_features.rename(columns={'preprocessed_text_after_features': 'TEXT'}, inplace=True)

# Rearrange the columns to match the desired order
final_columns_order = ['TEXT', 'AGGRESSION_CLASS', 'LABEL', 'abusive_word_count','num_tokens', 'post_size', 'has_urls', 'has_phone_numbers', 'has_hashtags', 'single_letter_count', 'avg_word_length', 'uppercase_word_count', 'punctuation_count']
concated_train_final = concated_train_with_features[final_columns_order]

# Now your DataFrame 'concated_train_final' should have the desired structure



# Create a new DataFrame to store the extracted features



concated_train_final[concated_train_final['abusive_word_count'] != 0]

# Create a new DataFrame to store the extracted features
extracted_features_valid = pd.DataFrame()

for index, row in concated_valid.iterrows():
    preprocessed_text = row['TEXT']
    features = extract_features(preprocessed_text)

    # Append the features dictionary to the DataFrame
    extracted_features_valid = pd.concat([extracted_features_valid, pd.DataFrame([features])], ignore_index=True)

# Concatenate the extracted features DataFrame with the original DataFrame
concated_valid_with_features = pd.concat([concated_valid, extracted_features_valid], axis=1)

# Preprocess the text after adding feature columns
for index, row in concated_valid_with_features.iterrows():
    preprocessed_text_after_features = preprocess_text(row['TEXT'])
    concated_valid_with_features.at[index, 'preprocessed_text_after_features'] = preprocessed_text_after_features

# Drop the original 'TEXT' column
concated_valid_with_features.drop(columns=['TEXT'], inplace=True)

# Rename the 'preprocessed_text_after_features' column to 'TEXT'
concated_valid_with_features.rename(columns={'preprocessed_text_after_features': 'TEXT'}, inplace=True)

# Rearrange the columns to match the desired order
final_columns_order = ['TEXT', 'AGGRESSION_CLASS', 'LABEL','abusive_word_count' ,'num_tokens', 'post_size', 'has_urls', 'has_phone_numbers', 'has_hashtags', 'single_letter_count', 'avg_word_length', 'uppercase_word_count', 'punctuation_count']
concated_valid_final = concated_valid_with_features[final_columns_order]

# Now your DataFrame 'concated_train_final' should have the desired structure

concated_valid_final

import re
import string
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import tensorflow as tf
from tensorflow.keras import layers

scaler = StandardScaler()
num_features = ['abusive_word_count','num_tokens', 'post_size', 'has_urls', 'has_phone_numbers', 'has_hashtags',
                'single_letter_count', 'avg_word_length', 'uppercase_word_count', 'punctuation_count']

# Transform the training data
concated_train_final[num_features] = scaler.fit_transform(concated_train_final[num_features])

# Transform the validation data
concated_valid_final[num_features] = scaler.transform(concated_valid_final[num_features])

concated_train_final

max_length = 160 #change
num_words = 4000
tokenizer = Tokenizer(num_words=num_words)
tokenizer.fit_on_texts(concated_train_final['TEXT'])
X_train_text = tokenizer.texts_to_sequences(concated_train_final['TEXT'])
X_valid_text = tokenizer.texts_to_sequences(concated_valid_final['TEXT'])

X_train_text = pad_sequences(X_train_text, maxlen=max_length,padding='post')
X_valid_text = pad_sequences(X_valid_text, maxlen=max_length,padding = 'post')

print(len(tokenizer.word_index.ite))

token_lengths = [len(tokens) for tokens in X_train_text]

sns.distplot(token_lengths)
plt.xlim([0, 256]);
plt.xlabel('Token count');

import gensim.downloader as api
glove_gensim = api.load('glove-wiki-gigaword-100')



vector_size = 100
gensim_weight_matrix = np.zeros((num_words, vector_size))

for word, index in tokenizer.word_index.items():
    if index < num_words:
        if word in glove_gensim.key_to_index:  # Check if the word exists in the vocabulary
            gensim_weight_matrix[index] = glove_gensim[word]
        else:
            gensim_weight_matrix[index] = np.zeros(vector_size)



gensim_weight_matrix.shape

len(tokenizer.word_index)



#tokenizer.word_index

X_train_features = concated_train_final.drop(columns=['LABEL','TEXT','AGGRESSION_CLASS'])
X_valid_features = concated_valid_final.drop(columns=['LABEL','TEXT','AGGRESSION_CLASS'])


y_train_labels = concated_train_final['LABEL']
y_valid_labels = concated_valid_final['LABEL']



num_classes = len(np.unique(y_train_labels))
print(num_classes)
y_train_encoded = to_categorical(y_train_labels, num_classes=num_classes)

y_valid_encoded = to_categorical(y_valid_labels, num_classes=num_classes)

print(X_train_text.shape, X_train_features.shape, y_train_encoded.shape)

print(X_valid_text.shape, X_valid_features.shape, y_train_encoded.shape)



import matplotlib.pyplot as plt

def plot_training_history(history):
    # Extract accuracy and loss values from the history object
    train_acc = history.history['accuracy']
    val_acc = history.history['val_accuracy']
    train_loss = history.history['loss']
    val_loss = history.history['val_loss']

    # Plot accuracy
    plt.figure(figsize=(12, 4))
    plt.subplot(1, 2, 1)
    plt.plot(range(1, len(train_acc) + 1), train_acc, label='Train Accuracy')
    plt.plot(range(1, len(val_acc) + 1), val_acc, label='Validation Accuracy')
    plt.title('Training and Validation Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()

    # Plot loss
    plt.subplot(1, 2, 2)
    plt.plot(range(1, len(train_loss) + 1), train_loss, label='Train Loss')
    plt.plot(range(1, len(val_loss) + 1), val_loss, label='Validation Loss')
    plt.title('Training and Validation Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    # Display the plot
    plt.tight_layout()
    plt.show()

from sklearn.metrics import classification_report
def get_classification_report(true_labels, predicted_labels, target_names):
    if len(true_labels.shape) > 1:
        true_labels_decoded = np.argmax(true_labels, axis=1)
    else:
        true_labels_decoded = true_labels

    if len(predicted_labels.shape) > 1:
        predicted_labels_decoded = np.argmax(predicted_labels, axis=1)
    else:
        predicted_labels_decoded = predicted_labels

    report = classification_report(true_labels_decoded, predicted_labels_decoded, target_names=target_names)
    return report

def predict_class(new_sentence, model, tokenizer, max_length, use_numerical_features=False):
    # Preprocess the new sentence
    preprocessed_sentence = preprocess_text(new_sentence)

    # Tokenize and pad the preprocessed sentence
    new_sentence_sequence = tokenizer.texts_to_sequences([preprocessed_sentence])
    new_sentence_sequence_padded = pad_sequences(new_sentence_sequence, maxlen=max_length)

    if use_numerical_features:
        # Prepare the numerical features
        new_sentence_features = extract_features(preprocessed_sentence)

        # Reshape the numerical features
        new_sentence_features_reshaped = np.array(list(new_sentence_features.values())).reshape(1, -1)

        # Predict the class of the new sentence
        predictions = model.predict([new_sentence_sequence_padded, new_sentence_features_reshaped])
    else:
        # Predict the class of the new sentence
        predictions = model.predict(new_sentence_sequence_padded)

    predicted_class = np.argmax(predictions, axis=1)

    return predicted_class

epochs = 20
emb_dim = 100
batch_size = 128
#RNN_CELL_SIZE = 32
vocab_size = 22000
class_names = ['NAG', 'CAG', 'OAG']

"""## CNN"""

from tensorflow.keras import regularizers

# Define the input layers
text_input = layers.Input(shape=(max_length,), name='text_input')
features_input = layers.Input(shape=(len(num_features),), name='features_input')

# Embedding layer for text input
embedding_layer = Embedding(
    input_dim=vocab_size,
    output_dim=100,
    weights=[gensim_weight_matrix],
    input_length=max_length,
    trainable=False
)(text_input)

text_conv1d = layers.Conv1D(
    filters=250,                       #100
    kernel_size=3,
    activation='relu',
    strides =1,
    kernel_regularizer=regularizers.l2(0.01)  # Add L2 regularization
)(embedding_layer)
text_pooling_2 = layers.GlobalMaxPooling1D()(text_conv1d)

# Dense layer for features input
features_dense = layers.Dense(
    250,
    activation='relu',
    kernel_regularizer=regularizers.l2(0.01)  # Add L2 regularization
)(features_input)

# Concatenate the text and features layers
concatenated = layers.concatenate([text_pooling_2, features_dense])

# Output layer
output = layers.Dense(num_classes, activation='softmax')(concatenated)

# Create the model
cnn_model = tf.keras.Model(inputs=[text_input, features_input], outputs=output)

# Compile the model
cnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Print the model summary
cnn_model.summary()

# Train the model
cnn_history = cnn_model.fit(
    [X_train_text, X_train_features],
    y_train_encoded,
    epochs=20,
    batch_size=batch_size,
    validation_data=([X_valid_text, X_valid_features], y_valid_encoded)
)

plot_training_history(cnn_history)

loss, accuracy = cnn_model.evaluate([X_valid_text, X_valid_features], y_valid_encoded)

# Print the validation loss and accuracy
print(f'Validation Loss: {loss:.4f}, Validation Accuracy: {accuracy:.4f}')

class_names = ['NAG','CAG','OAG']

y_pred_probs = cnn_model.predict(([X_valid_text, X_valid_features]))
y_pred_classes = np.argmax(y_pred_probs, axis=1)

# Convert one-hot encoded labels to class indices
y_true = np.argmax(y_valid_encoded,axis = 1)

# Generate the classification report
class_report = classification_report(y_true, y_pred_classes, target_names=class_names)

print("Classification Report:\n", class_report)

new_sentence = "Shame on Modi Government .... Must teach lesson to Pakistan now.. no use of demonetization and all.. its time to attack.."
preprocessed_sentence = preprocess_text(new_sentence)  # Apply the necessary preprocessing steps to the new sentence

# Tokenize and pad the preprocessed sentence
new_sentence_sequence = tokenizer.texts_to_sequences([preprocessed_sentence])
new_sentence_sequence_padded = pad_sequences(new_sentence_sequence, maxlen=max_length)

# Prepare the numerical features
new_sentence_features = extract_features(preprocessed_sentence)  # Implement a function to extract features from the new sentence

# Reshape the numerical features
new_sentence_features_reshaped = np.array(list(new_sentence_features.values())).reshape(1, -1)  # Reshape the features to match the model's input shape

# Predict the class of the new sentence
predictions = cnn_model.predict([new_sentence_sequence_padded, new_sentence_features_reshaped])

predicted_class = np.argmax(predictions, axis=1)

# Print the predicted class
print(f"The predicted class for the new sentence is: {predicted_class}")

"""## LSTM"""

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, concatenate

# Define input layers
text_input = Input(shape=(max_length,))
features_input = Input(shape=(X_train_features.shape[1],))

# Embedding layer for text input
embedding_layer = Embedding(input_dim=vocab_size,output_dim=emb_dim)(text_input)

# LSTM layer for text input
lstm_units = 196
lstm_output = LSTM(units=lstm_units,dropout=0.2, recurrent_dropout=0.2)(embedding_layer)

# Concatenate LSTM outputs with numerical features
merged = concatenate([lstm_output, features_input])

# Dense layers
dense_units = 400
dense_output = Dense(units=dense_units, activation='relu')(merged)
output = Dense(units=num_classes, activation='softmax')(dense_output)

# Create the model
lstm_model = Model(inputs=[text_input, features_input], outputs=output)

# Compile the model
lstm_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Fit the model
history=lstm_model.fit(x=[X_train_text, X_train_features], y=y_train_encoded, validation_data=([X_valid_text, X_valid_features], y_valid_encoded), epochs=10, batch_size=batch_size)

loss, accuracy = model.evaluate([X_valid_text, X_valid_features], y_valid_encoded)

# Print the validation loss and accuracy
print(f'Validation Loss: {loss:.4f}, Validation Accuracy: {accuracy:.4f}')



plot_training_history(history)

class_names = ['NAG','CAG','OAG']

y_pred_probs = model.predict(([X_valid_text, X_valid_features]))
y_pred_classes = np.argmax(y_pred_probs, axis=1)

# Convert one-hot encoded labels to class indices
y_true = np.argmax(y_valid_encoded,axis = 1)

# Generate the classification report
class_report = classification_report(y_true, y_pred_classes, target_names=class_names)

print("Classification Report:\n", class_report)

predicted_labels

new_sentence = "when rahul gandhi was in line for exchange,it was said to be for publicity, here mother of our respected PM is in line,what to say now?"
preprocessed_sentence = preprocess_text(new_sentence)  # Apply the necessary preprocessing steps to the new sentence

# Tokenize and pad the preprocessed sentence
new_sentence_sequence = tokenizer.texts_to_sequences([preprocessed_sentence])
new_sentence_sequence_padded = pad_sequences(new_sentence_sequence, maxlen=max_length)

# Prepare the numerical features
new_sentence_features = extract_features(preprocessed_sentence)  # Implement a function to extract features from the new sentence

# Reshape the numerical features
new_sentence_features_reshaped = np.array(list(new_sentence_features.values())).reshape(1, -1)  # Reshape the features to match the model's input shape

# Predict the class of the new sentence
predictions = model.predict([new_sentence_sequence_padded, new_sentence_features_reshaped])

predicted_class = np.argmax(predictions, axis=1)

# Print the predicted class
print(f"The predicted class for the new sentence is: {predicted_class}")



from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM, Dense, concatenate

# Define input layers
text_input = Input(shape=(max_length,))
features_input = Input(shape=(X_train_features.shape[1],))

# Embedding layer for text input
embedding_layer = Embedding(input_dim=vocab_size, weights=[gensim_weight_matrix], output_dim=emb_dim)(text_input)

# Bidirectional LSTM layer for text input
lstm_units = 100
bi_lstm_output = Bidirectional(LSTM(units=lstm_units,dropout=0.2, recurrent_dropout=0.2))(embedding_layer)

# Concatenate BiLSTM outputs with numerical features
merged = concatenate([bi_lstm_output, features_input])

# Dense layers
dense_units = 64
dense_output = Dense(units=dense_units, activation='relu')(merged)
output = Dense(units=num_classes, activation='softmax')(dense_output)

# Create the model
model = Model(inputs=[text_input, features_input], outputs=output)

# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Fit the model
history = model.fit(x=[X_train_text, X_train_features], y=y_train_encoded, validation_data=([X_valid_text, X_valid_features], y_valid_encoded), epochs=10, batch_size=batch_size)

