# -*- coding: utf-8 -*-
"""NB.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pe9waFlnlekNN_yTs2yMKUJ4uFqb5CUr
"""

pip install emoji

pip install indic-transliteration

pip install wordsegment

pip install contractions

import re
import emoji
import wordsegment
from wordsegment import load, segment
load()
import numpy as np
import pandas as pd

import nltk
from nltk.corpus import stopwords
import contractions
from sklearn.metrics import classification_report, confusion_matrix
import nltk
nltk.download('wordnet')
from nltk.stem import WordNetLemmatizer

import seaborn as sns

nltk.download('stopwords')


from sklearn.pipeline import make_pipeline

from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.pipeline import make_pipeline


import re
import string
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import tensorflow as tf
from tensorflow.keras import layers

train = pd.read_csv('/content/drive/MyDrive/agr_en_train.csv', names = ['SOURCE', 'TEXT', 'AGGRESSION_CLASS'])
valid =  pd.read_csv('/content/drive/MyDrive/agr_en_dev.csv', names = ['SOURCE', 'TEXT', 'AGGRESSION_CLASS'])

train = train.sample(frac=0.5, random_state=42)  # You can change the random_state if you want a different random sampling
valid= valid.sample(frac=0.5, random_state=42)

len(train)

import matplotlib.pyplot as plt
sentiment_count = train["AGGRESSION_CLASS"].value_counts()
plt.pie(sentiment_count, labels=sentiment_count.index,
        autopct='%1.1f%%', shadow=True, startangle=140)
plt.show()

from wordcloud import WordCloud

pos_tweets = train[train["AGGRESSION_CLASS"]=="OAG"]
txt = " ".join(tweet.lower() for tweet in pos_tweets["TEXT"])
wordcloud = WordCloud().generate(txt)
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
plt.show()

pos_tweets = train[train["AGGRESSION_CLASS"]=="CAG"]
txt = " ".join(tweet.lower() for tweet in pos_tweets["TEXT"])
wordcloud = WordCloud().generate(txt)
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
plt.show()

np.random.seed(42)
train_shuffled = train.reindex(np.random.permutation(train.index))
valid_shuffled = valid.reindex(np.random.permutation(valid.index))

CAG = train_shuffled[train_shuffled['AGGRESSION_CLASS'] == 'CAG']
OAG = train_shuffled[train_shuffled['AGGRESSION_CLASS'] == 'OAG']
NAG = train_shuffled[train_shuffled['AGGRESSION_CLASS'] == 'NAG']

concated_train = pd.concat([CAG, OAG, NAG], ignore_index=True)
concated_train['LABEL'] = 0

np.random.seed(42)
concated_train = concated_train.reindex(np.random.permutation(concated_train.index))


# Class to idx
concated_train.loc[concated_train['AGGRESSION_CLASS'] == 'CAG', 'LABEL'] = 1
concated_train.loc[concated_train['AGGRESSION_CLASS'] == 'OAG', 'LABEL'] = 2
concated_train.loc[concated_train['AGGRESSION_CLASS'] == 'NAG', 'LABEL'] = 0

# Valid
CAG = valid_shuffled[valid_shuffled['AGGRESSION_CLASS'] == 'CAG']
OAG = valid_shuffled[valid_shuffled['AGGRESSION_CLASS'] == 'OAG']
NAG = valid_shuffled[valid_shuffled['AGGRESSION_CLASS'] == 'NAG']

concated_valid = pd.concat([CAG, OAG, NAG], ignore_index=True)
concated_valid['LABEL'] = 0

np.random.seed(42)
concated_valid = concated_valid.reindex(np.random.permutation(concated_valid.index))






# Class to idx
concated_valid.loc[concated_valid['AGGRESSION_CLASS'] == 'CAG', 'LABEL'] = 1
concated_valid.loc[concated_valid['AGGRESSION_CLASS'] == 'OAG', 'LABEL'] = 2
concated_valid.loc[concated_valid['AGGRESSION_CLASS'] == 'NAG', 'LABEL'] = 0

concated_train

'''
def preprocess_text(text):
  text = text.lower()

    # Replace URL with http
  text = re.sub(r'http\S+', 'http', text)

  # Replace emoticons with corresponding phrases
  text = emoji.emojize(text, language='en')
  text = emoji.demojize(text, language='en', delimiters=(' ', ' '))

  # Segment hashtags
  text = re.sub(r'#(\w+)', lambda m: ' '.join(segment(m.group(1))), text)

  #remove extra white spaces
  text = " ".join(text.split())

  #word_Contractions
  text = contractions.fix(text)

  # Tokenize text
  words = text.split()

  # Remove stop words
  stop_words = set(stopwords.words('english'))
  words = [word for word in words if word.lower() not in stop_words]


  #lemmatize
  lemmatizer = WordNetLemmatizer()
  words = [lemmatizer.lemmatize(word) for word in words]
  words = [lemmatizer.lemmatize(word, "v") for word in words]

  # Remove digits, punctuation, email, and non UTF-8 characters
  words = [re.sub(r'\d+', '', word) for word in words]
  words = [re.sub(r'[^\w\s]+', '', word) for word in words]
  words = [re.sub(r'\S+@\S+', '', word) for word in words]

  # Encode and decode to handle non-UTF-8 characters
  words = [word.encode('ascii', 'ignore').decode('utf-8') for word in words]

  return ' '.join(words)
  '''


from indic_transliteration import sanscript
from indic_transliteration.sanscript import transliterate

def preprocess_text(text):
  text = text.lower()
  #transliterator = Transliterator(source='hi', target='en', build_lookup=True)
    # Replace URL with http
  text = re.sub(r'http\S+', 'http', text)

  # Replace emoticons with corresponding phrases
  text = emoji.emojize(text, language='en')
  text = emoji.demojize(text, language='en', delimiters=(' ', ' '))

  # Segment hashtags
  text = re.sub(r'#(\w+)', lambda m: ' '.join(segment(m.group(1))), text)

  #remove extra white spaces
  text = " ".join(text.split())

  #word_Contractions
  text = contractions.fix(text)

  # Tokenize text
  words = text.split()

  # Remove stop words
  stop_words = set(stopwords.words('english'))
  words = [word for word in words if word.lower() not in stop_words]


  #lemmatize
  lemmatizer = WordNetLemmatizer()
  words = [lemmatizer.lemmatize(word) for word in words]
  words = [lemmatizer.lemmatize(word, "v") for word in words]

  # Remove digits, punctuation, email, and non UTF-8 characters
  words = [re.sub(r'\d+', '', word) for word in words]
  words = [re.sub(r'[^\w\s]+', '', word) for word in words]
  words = [re.sub(r'\S+@\S+', '', word) for word in words]

  # Transliterate Hindi text to Roman
  for i, word in enumerate(words):
    if any(c in sanscript.DEVANAGARI for c in word):
      roman_word = transliterate(word, sanscript.DEVANAGARI, sanscript.ITRANS)
      words[i] = roman_word

  sentence = ' '.join(words)

  # Transliterate Hindi text to Roman
  roman_text = transliterate(sentence, sanscript.DEVANAGARI, sanscript.ITRANS)

  # Encode and decode to handle non-UTF-8 characters
  roman_text = roman_text.encode('ascii', 'ignore').decode('utf-8')

  return roman_text

import string
def extract_features(preprocessed_text):
    # Preprocess the input text
    #preprocessed_text = preprocess_text(input_text)

    # Count of abusive/aggressive/offensive words
    #abusive_words = ["abusive_word1", "abusive_word2", "abusive_word3"]  # Replace with your list of abusive words
    #abusive_word_count = sum(1 for word in preprocessed_text.split() if word in abusive_words)

    # Number of tokens
    num_tokens = len(preprocessed_text.split())

    # Size of post/comment (number of characters)
    post_size = len(preprocessed_text)

    # Presence of URLs
    has_urls = 1 if re.search(r'http\S+', preprocessed_text) else 0

    # Presence of phone numbers (a simple example)
    has_phone_numbers = 1 if re.search(r'\d{10}', preprocessed_text) else 0

    # Presence of hash-tags
    has_hashtags = 1 if re.search(r'#\w+', preprocessed_text) else 0

    # Number of single letters
    single_letter_count = sum(1 for word in preprocessed_text.split() if len(word) == 1)

    # Average length of words
    words = preprocessed_text.split()
    avg_word_length = sum(len(word) for word in words) / len(words)

    # Number of words with uppercase characters
    uppercase_word_count = sum(1 for word in words if any(c.isupper() for c in word))

    # Number of Punctuation
    punctuation_count = sum(1 for char in preprocessed_text if char in string.punctuation)

    # ... add more features ...

    # Return the extracted features as a list
    return {
        "num_tokens": num_tokens,
        "post_size": post_size,
        "has_urls": has_urls,
        "has_phone_numbers": has_phone_numbers,
        "has_hashtags": has_hashtags,
        "single_letter_count": single_letter_count,
        "avg_word_length": avg_word_length,
        "uppercase_word_count": uppercase_word_count,
        "punctuation_count": punctuation_count,
        # ... add more features ...
    }

# Create a new DataFrame to store the extracted features
extracted_features_train = pd.DataFrame()

for index, row in concated_train.iterrows():
    preprocessed_text = row['TEXT']
    features = extract_features(preprocessed_text)

    # Append the features dictionary to the DataFrame
    extracted_features_train = pd.concat([extracted_features_train, pd.DataFrame([features])], ignore_index=True)

# Concatenate the extracted features DataFrame with the original DataFrame
concated_train_with_features = pd.concat([concated_train, extracted_features_train], axis=1)

# Preprocess the text after adding feature columns
for index, row in concated_train_with_features.iterrows():
    preprocessed_text_after_features = preprocess_text(row['TEXT'])
    concated_train_with_features.at[index, 'preprocessed_text_after_features'] = preprocessed_text_after_features

# Drop the original 'TEXT' column
concated_train_with_features.drop(columns=['TEXT'], inplace=True)

# Rename the 'preprocessed_text_after_features' column to 'TEXT'
concated_train_with_features.rename(columns={'preprocessed_text_after_features': 'TEXT'}, inplace=True)

# Rearrange the columns to match the desired order
final_columns_order = ['TEXT', 'AGGRESSION_CLASS', 'LABEL', 'num_tokens', 'post_size', 'has_urls', 'has_phone_numbers', 'has_hashtags', 'single_letter_count', 'avg_word_length', 'uppercase_word_count', 'punctuation_count']
concated_train_final = concated_train_with_features[final_columns_order]

# Now your DataFrame 'concated_train_final' should have the desired structure



# Create a new DataFrame to store the extracted features

# Create a new DataFrame to store the extracted features
extracted_features_valid = pd.DataFrame()

for index, row in concated_valid.iterrows():
    preprocessed_text = row['TEXT']
    features = extract_features(preprocessed_text)

    # Append the features dictionary to the DataFrame
    extracted_features_valid = pd.concat([extracted_features_valid, pd.DataFrame([features])], ignore_index=True)

# Concatenate the extracted features DataFrame with the original DataFrame
concated_valid_with_features = pd.concat([concated_valid, extracted_features_valid], axis=1)

# Preprocess the text after adding feature columns
for index, row in concated_valid_with_features.iterrows():
    preprocessed_text_after_features = preprocess_text(row['TEXT'])
    concated_valid_with_features.at[index, 'preprocessed_text_after_features'] = preprocessed_text_after_features

# Drop the original 'TEXT' column
concated_valid_with_features.drop(columns=['TEXT'], inplace=True)

# Rename the 'preprocessed_text_after_features' column to 'TEXT'
concated_valid_with_features.rename(columns={'preprocessed_text_after_features': 'TEXT'}, inplace=True)

# Rearrange the columns to match the desired order
final_columns_order = ['TEXT', 'AGGRESSION_CLASS', 'LABEL', 'num_tokens', 'post_size', 'has_urls', 'has_phone_numbers', 'has_hashtags', 'single_letter_count', 'avg_word_length', 'uppercase_word_count', 'punctuation_count']
concated_valid_final = concated_valid_with_features[final_columns_order]

# Now your DataFrame 'concated_train_final' should have the desired structure

'''
concated_train['TEXT'] = concated_train['TEXT'].apply(preprocess_text)
concated_valid['TEXT'] = concated_valid['TEXT'].apply(preprocess_text)
'''

concated_train_final



nltk.download('tagsets')



concated_train_cp.head()

import nltk
from nltk.tokenize import word_tokenize
nltk.download('punkt')

def tokenize(tweet):
  tokens = word_tokenize(tweet)
  return tokens

vectorizer = CountVectorizer(tokenizer=tokenize)
X_train_text = vectorizer.fit_transform(concated_train_final["TEXT"])


X_valid_text = vectorizer.transform(concated_valid_final["TEXT"])


vocab_size = len(vectorizer.vocabulary_)
print("Vocabulary Size:", vocab_size)

feature_names = vectorizer.get_feature_names_out()

# Print the first 20 feature names
print("First 20 feature names:")
print(feature_names[:20])

# Print the total number of features
print("Total number of features:", len(feature_names))

from sklearn.feature_extraction import DictVectorizer
num_features_train = concated_train_final.drop(columns=["TEXT", "AGGRESSION_CLASS", "LABEL"])

num_features_valid = concated_valid_final.drop(columns=["TEXT", "AGGRESSION_CLASS", "LABEL"])
#dict_vectorizer = DictVectorizer()

#X_num = dict_vectorizer.fit_transform(num_features.to_dict(orient="records"))

from sklearn.preprocessing import StandardScaler

X_num_train = num_features_train.values  # Convert DataFrame to numpy array
X_num_valid = num_features_valid.values
# Initialize the scaler
scaler = StandardScaler()  # or MinMaxScaler()

# Fit and transform the numerical features
#X_train_scaled = scaler.fit_transform(X_num_train)
#X_valid_scaled = scaler.transform(X_num_valid)



from scipy.sparse import hstack

# Convert the sparse matrix to a dense array
X_train_text_dense = X_train_text.toarray()

# Convert the validation sparse matrix to a dense array
X_valid_text_dense = X_valid_text.toarray()

# Concatenate the dense text features with the scaled numerical features
X_train_combined = np.concatenate((X_train_text_dense, X_num_train), axis=1)
X_valid_combined = np.concatenate((X_valid_text_dense, X_num_valid), axis=1)

X_train_combined.shape

concated_train_final['LABEL'].shape





from sklearn.naive_bayes import MultinomialNB

# Initialize the Multinomial Naive Bayes model
nb_classifier = MultinomialNB()

# Train the model on the training data
nb_classifier.fit(X_train_combined, concated_train_final['LABEL'])

# Make predictions on the validation data
# Convert to dense array if needed
predictions = nb_classifier.predict(X_valid_combined)
print(predictions[1:20])
# Evaluate the model's performance
from sklearn.metrics import accuracy_score, classification_report

accuracy = accuracy_score(concated_valid_final['LABEL'], predictions)
print("Accuracy:", accuracy)

report = classification_report(concated_valid_final['LABEL'], predictions)
print("Classification Report:\n", report)

def show_confusion_matrix(confusion_matrix):
  hmap = sns.heatmap(confusion_matrix, annot=True, fmt="d", cmap="Blues")
  hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')
  hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')
  plt.ylabel('True sentiment')
  plt.xlabel('Predicted sentiment');

class_names = ['NAG',"CAG","OAG"]
cm = confusion_matrix(concated_valid['LABEL'], predictions)
df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)
show_confusion_matrix(df_cm)

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score

# Initialize the Random Forest classifier
rf_classifier = RandomForestClassifier(
    class_weight="balanced",
    max_features="sqrt",
    n_estimators=60,
    min_weight_fraction_leaf=0.0,
    criterion="entropy",
    random_state=2
)  # Adjust parameters as needed

# Train the Random Forest classifier on the entire training data
rf_classifier.fit(X_train_combined, concated_train_final['LABEL'])

# Make predictions on the validation data
rf_predictions = rf_classifier.predict(X_valid_combined)

feature_importances = rf_classifier.feature_importances_

# Number of features used
num_features_used = np.sum(feature_importances > 0)
print("Number of Features Used by Random Forest:", num_features_used)

# Evaluate the Random Forest model's performance
rf_accuracy = accuracy_score(concated_valid_final['LABEL'], rf_predictions)
print("Random Forest Accuracy:", rf_accuracy)

rf_report = classification_report(concated_valid_final['LABEL'], rf_predictions)
print("Random Forest Classification Report:\n", rf_report)







from sklearn.metrics import accuracy_score
from sklearn.ensemble import  RandomForestClassifier, VotingClassifier

nb_pipeline = make_pipeline(MultinomialNB(alpha=1))
rf_pipeline = make_pipeline(RandomForestClassifier())

voting_model = VotingClassifier(estimators=[('nb', nb_pipeline), ('rf', rf_pipeline)], voting='soft')


for  clf in (nb_pipeline,rf_pipeline,voting_model):
  clf.fit(X_train_combined, concated_train_final['LABEL'])
  predicted_labels =clf.predict(X_valid_combined)

  print(f"Accuracy for {clf.__class__.__name__}", accuracy_score(concated_valid['LABEL'], predicted_labels))
  print(classification_report(concated_valid['LABEL'], predicted_labels))

txt = ["Stupid economist and stupid PM, who cannot do better for country"]
txt = preprocess_text(txt[0])

prediction = clf.predict([txt])
print("Prediction:", prediction)











def show_confusion_matrix(confusion_matrix):
  hmap = sns.heatmap(confusion_matrix, annot=True, fmt="d", cmap="Blues")
  hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')
  hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')
  plt.ylabel('True sentiment')
  plt.xlabel('Predicted sentiment');

class_names = ['NAG',"CAG","OAG"]
cm = confusion_matrix(concated_valid['LABEL'], predicted_labels)
df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)
show_confusion_matrix(df_cm)



from sklearn.metrics import f1_score
weighted_f1 = f1_score(concated_valid['LABEL'], predicted_labels, average='weighted')
print("Weighted F1 Score:", weighted_f1)

txt = ["Stupid economist and stupid PM, who cannot do better for country"]
txt = preprocess_text(txt[0])
prediction = model.predict([txt])
print("Prediction:", prediction)



from sklearn.pipeline import make_pipeline
from sklearn.ensemble import RandomForestClassifier

# Create a pipeline with TfidfVectorizer and Random Forest
model = make_pipeline(TfidfVectorizer(tokenizer=tokenize), RandomForestClassifier())

# Fit the model
model.fit(concated_train['TEXT'], concated_train['LABEL'])

# Predict labels for validation data
predicted_labels = model.predict(concated_valid['TEXT'])

# Evaluate the model
print(classification_report(concated_valid['LABEL'], predicted_labels))
confusion = confusion_matrix(concated_valid['LABEL'], predicted_labels,target_names=class_names)
print(confusion)

from sklearn.metrics import accuracy_score

nb_pipeline = make_pipeline(CountVectorizer(tokenizer=tokenize), MultinomialNB(alpha=1))
rf_pipeline = make_pipeline(CountVectorizer(tokenizer=tokenize), RandomForestClassifier())

voting_model = VotingClassifier(estimators=[('nb', nb_pipeline), ('rf', rf_pipeline)], voting='soft')


for  clf in (nb_pipeline,rf_pipeline,voting_model):
  clf.fit(concated_train['TEXT'], concated_train['LABEL'])
  predicted_labels =clf.predict(concated_valid['TEXT'])

  print(f"Accuracy for {clf.__class__.__name__}", accuracy_score(concated_valid['LABEL'], predicted_labels))
  print(classification_report(concated_valid['LABEL'], predicted_labels))



txt = ["He is real culprits for corruption in india during congress ruling.. He should be hang till death... one of weakest PM India ever had.."]
txt = preprocess_text(txt[0])
prediction = clf.predict([txt])
print(prediction)

test = pd.read_csv('/content/drive/MyDrive/agr_en_fb_gold.csv',names =['SOURCE','TEXT','AGGRESSION_CLASS'])

test=test[['TEXT','AGGRESSION_CLASS']]
test['LABEL'] = 0
test.loc[test['AGGRESSION_CLASS'] == 'CAG', 'LABEL'] = 1
test.loc[test['AGGRESSION_CLASS'] == 'OAG', 'LABEL'] = 2
test.loc[test['AGGRESSION_CLASS'] == 'NAG', 'LABEL'] = 0
test['TEXT']= test['TEXT'].apply(preprocess_text)

test_predict = clf.predict(test['TEXT'])

# Generate the classification report
class_report = classification_report(test['LABEL'], test_predict)

print("Classification Report:\n", class_report)

test.head()

